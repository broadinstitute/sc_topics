{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scanpy as sc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.random.mtrand import RandomState\n",
    "from sklearn.utils import check_random_state, check_array\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1. Use raw count data\n",
    "  2. Use normalized data\n",
    "  3. Use tfidf (typical preprocessing for NLP)\n",
    "  4. Oana's preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Count Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oana's Preprocessing\n",
    "\n",
    "\n",
    "**slack convo**\n",
    "\n",
    "__oana  11:34 AM__\n",
    "Hi Kirk, I have a naive question about topic models.\n",
    "I know that the typical way people run the topic models is to take either the normalized counts, or the log1p rounded to an integer. I wonder how ok it would be to take the scaled values, set everything that's negative to 0, and then round the positive ones. I've done that and gotten results that make sense, but it doesn't feel mathematically sound. I wonder if you might have some insights about this.\n",
    "\n",
    "__kirkgosik  2:49 PM__\n",
    "are you basically getting 0s, and 1s then?\n",
    "2:50\n",
    "they way I would think about that would be like asking the question, is this gene expressed above average in the dataset?\n",
    "2:52\n",
    "I guess you would be getting the count you feed in to be the number of standard deviations you would be above the average expression for each gene\n",
    "2:54\n",
    "I would think that would do a similiar thing as it does in PCA, if you don't scale highly expressed genes dominant if you do scale they get more evenly represented\n",
    "2:55\n",
    "i would think it doesn't mathematically affect the algorithm of LDA but it would just impact how you interpret the results and what might come out as meaningful (edited) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
